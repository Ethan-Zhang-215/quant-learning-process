# -*- coding: utf-8 -*-
"""
æ–‡æœ¬æƒ…ç»ªåˆ†ç±»å®Œæ•´ä»£ç ï¼ˆPyTorchç‰ˆï¼‰
åŠŸèƒ½ï¼šè®­ç»ƒCNN/LSTMæ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œæ­£é¢/ä¸­æ€§/è´Ÿé¢ä¸‰åˆ†ç±»
"""
import pandas as pd
import re
import torch
import torch.nn as nn
import torch.nn.functional as F  # æ·»åŠ è¿™è¡Œå¯¼å…¥
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
#=================== 0. display =======================
# åŠ è½½æ•°æ®
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

# æŸ¥çœ‹æ•°æ®
print(train_data.head())
print(f"è®­ç»ƒé›†å¤§å°: {train_data.shape}")
print(f"æµ‹è¯•é›†å¤§å°: {test_data.shape}")

# æŸ¥çœ‹æƒ…æ„Ÿåˆ†å¸ƒ
print(train_data['sentiment'].value_counts())

# ==================== 1. æ•°æ®é¢„å¤„ç† ====================
def clean_text(text):
    """æ–‡æœ¬æ¸…æ´—å‡½æ•°ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰"""
    if not isinstance(text, str):
        text = str(text)  # ç¡®ä¿è¾“å…¥ä¸ºå­—ç¬¦ä¸²
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™å­—æ¯å’Œç©ºæ ¼ï¼‰
    text = re.sub(r'http\S+|www\S+|@\w+|#\w+', '', text)  # å»URL/æ ‡ç­¾
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower().strip()  # ç»Ÿä¸€å°å†™
    # åˆ†è¯ä¸è¯å½¢è¿˜åŸ
    tokens = [WordNetLemmatizer().lemmatize(word)
              for word in text.split()
              if word not in set(stopwords.words('english'))]  # å»åœç”¨è¯
    return ' '.join(tokens)

# ğŸ¯ åŠ è½½æ•°æ®ï¼ˆå‡è®¾CSVåŒ…å«textå’Œsentimentåˆ—ï¼‰
train_data = pd.read_csv('train.csv').dropna(subset=['text', 'sentiment'])
test_data = pd.read_csv('test.csv').dropna(subset=['text', 'sentiment'])

# ğŸ¯ åº”ç”¨æ¸…æ´—ï¼ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‹¬ç«‹å¤„ç†ï¼‰
train_data['cleaned_text'] = train_data['text'].apply(clean_text)
test_data['cleaned_text'] = test_data['text'].apply(clean_text)

# ==================== 2. æ„å»ºè¯æ±‡è¡¨ ====================
def build_vocab(texts, max_size=10000):
    """æ ¹æ®è®­ç»ƒé›†æ„å»ºè¯æ±‡è¡¨ï¼ˆæµ‹è¯•é›†ä¸å‚ä¸ï¼‰"""
    word_counts = {}
    for text in texts:
        for word in text.split():
            word_counts[word] = word_counts.get(word, 0) + 1
    # æŒ‰è¯é¢‘æ’åºï¼Œå–å‰max_sizeä¸ªè¯
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    vocab = {word: idx + 2 for idx, (word, _) in enumerate(sorted_words[:max_size])}
    vocab["<pad>"] = 0  # å¡«å……æ ‡è®°
    vocab["<unk>"] = 1  # æœªçŸ¥è¯æ ‡è®°
    return vocab

# ğŸ¯ ä»…ç”¨è®­ç»ƒé›†æ„å»ºè¯æ±‡è¡¨ï¼ˆé˜²æ­¢æ•°æ®æ³„éœ²ï¼‰
vocab = build_vocab(train_data['cleaned_text'])
vocab_size = len(vocab)
print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")

# ==================== 3. æ•°æ®ç¼–ç ä¸åŠ è½½ ====================
class TextDataset(Dataset):

    def __init__(self, texts, labels, vocab, max_len=100):
        self.texts = texts  # æ–‡æœ¬æ•°æ®ï¼ˆpandas Seriesï¼‰
        self.labels = labels  # æ ‡ç­¾ï¼ˆnumpyæ•°ç»„ï¼‰
        self.vocab = vocab  # è¯æ±‡è¡¨å­—å…¸
        self.max_len = max_len  # ç»Ÿä¸€åºåˆ—é•¿åº¦

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # ğŸ¯ æ ¸å¿ƒæ“ä½œï¼šæ–‡æœ¬â†’æ•°å­—åºåˆ—â†’é•¿åº¦æ ‡å‡†åŒ–
        text = self.texts.iloc[idx]
        # å•è¯è½¬ç´¢å¼•ï¼ˆæœªçŸ¥è¯ç”¨<unk>ï¼‰
        sequence = [self.vocab.get(word, self.vocab["<unk>"])
                    for word in text.split()]
        # ç»Ÿä¸€é•¿åº¦ï¼ˆæˆªæ–­æˆ–å¡«å……ï¼‰
        if len(sequence) > self.max_len:
            sequence = sequence[:self.max_len]
        else:
            sequence = sequence + [self.vocab["<pad>"]] * (self.max_len - len(sequence))
        return torch.tensor(sequence), torch.tensor(self.labels[idx])


# ğŸ¯ æ ‡ç­¾ç¼–ç ï¼ˆæ­£é¢/ä¸­æ€§/è´Ÿé¢ â†’ 0/1/2ï¼‰
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_data['sentiment'])
test_labels = label_encoder.transform(test_data['sentiment'])

# ğŸ¯ åˆ›å»ºDatasetå’ŒDataLoader
train_dataset = TextDataset(train_data['cleaned_text'], train_labels, vocab)
test_dataset = TextDataset(test_data['cleaned_text'], test_labels, vocab)

batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)


# ==================== 4. æ¨¡å‹å®šä¹‰ ====================
class EmotionCNN(nn.Module):
    """CNNæ¨¡å‹ç»“æ„ï¼ˆé€‚åˆæ•æ‰å±€éƒ¨ç‰¹å¾ï¼‰"""
    def __init__(self, vocab_size, embed_dim=128, num_classes=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.conv1 = nn.Conv1d(embed_dim, 128, 5, padding=2)  # å·ç§¯æ ¸å¤§å°5
        self.bn1 = nn.BatchNorm1d(128)
        self.dropout = nn.Dropout(0.5)
        self.conv2 = nn.Conv1d(128, 64, 3, padding=1)  # å·ç§¯æ ¸å¤§å°3
        self.pool = nn.AdaptiveMaxPool1d(1)  # å…¨å±€æœ€å¤§æ± åŒ–å±‚
        self.fc = nn.Linear(64, num_classes)

    def forward(self, x):
        # æ•°æ®æµåŠ¨ï¼šåµŒå…¥å±‚ â†’ å·ç§¯ â†’ æ± åŒ– â†’ å…¨è¿æ¥
        x = self.embedding(x)  # [batch, seq_len, embed_dim]
        x = x.permute(0, 2, 1)  # è½¬ä¸º[batch, embed_dim, seq_len]é€‚åº”å·ç§¯
        x = self.dropout(F.relu(self.bn1(self.conv1(x))))
        x = F.relu(self.conv2(x))
        x = self.pool(x).squeeze(2)  # ä½¿ç”¨å®šä¹‰å¥½çš„æ± åŒ–å±‚
        return self.fc(x)


class EmotionLSTM(nn.Module):
    """LSTMæ¨¡å‹ç»“æ„ï¼ˆé€‚åˆåºåˆ—å»ºæ¨¡ï¼‰"""
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64, num_classes=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim,
                           batch_first=True,
                           bidirectional=True)
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(hidden_dim*2, num_classes)
        self.hidden_dim = hidden_dim  # å…³é”®ä¿®å¤ï¼šä¿å­˜ä¸ºç±»å±æ€§

    def forward(self, x):
        x = self.embedding(x)
        lstm_out, _ = self.lstm(x)  # è¾“å‡ºå½¢çŠ¶: [batch, seq_len, hidden_dim*2]
        # åŒå‘LSTMè¾“å‡ºå¤„ç†
        forward_last = lstm_out[:, -1, :self.hidden_dim]  # æ­£å‘æœ€åæ—¶é—´æ­¥
        backward_first = lstm_out[:, 0, self.hidden_dim:]  # åå‘ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥
        x = torch.cat([forward_last, backward_first], dim=1)
        x = self.dropout(x)
        return self.fc(x)


# ==================== 5. è®­ç»ƒä¸è¯„ä¼° ====================
def train_and_evaluate(model, model_name, train_loader, test_loader, epochs=15):
    """è®­ç»ƒæµç¨‹ï¼ˆå«éªŒè¯å’Œæ¨¡å‹ä¿å­˜ï¼‰"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    best_acc = 0
    history = {'train_loss': [], 'val_acc': []}

    for epoch in range(epochs):
        # ğŸ¯ è®­ç»ƒé˜¶æ®µï¼ˆä½¿ç”¨è®­ç»ƒé›†ï¼‰
        model.train()
        total_loss = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # ğŸ¯ éªŒè¯é˜¶æ®µï¼ˆä½¿ç”¨æµ‹è¯•é›†ï¼‰
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        val_acc = correct / total
        history['train_loss'].append(total_loss / len(train_loader))
        history['val_acc'].append(val_acc)

        # ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), f'best_{model_name}_model.pth')

        print(
            f'{model_name} Epoch {epoch + 1}/{epochs} | Loss: {total_loss / len(train_loader):.4f} | Val Acc: {val_acc:.4f}')

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.title(f'{model_name} Training Loss')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(history['val_acc'], label='Val Acc')
    plt.title(f'{model_name} Validation Accuracy')
    plt.legend()
    plt.savefig(f'{model_name}_training.png')
    plt.show()

    return best_acc


def full_evaluate(model, test_loader, label_encoder):
    """è¯¦ç»†è¯„ä¼°ï¼ˆè¾“å‡ºåˆ†ç±»æŠ¥å‘Šï¼‰"""
    device = next(model.parameters()).device
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.numpy())

    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds,
                                target_names=label_encoder.classes_))


# ==================== 6. æ‰§è¡Œè®­ç»ƒ ====================
print("\nè®­ç»ƒCNNæ¨¡å‹...")
cnn_model = EmotionCNN(vocab_size)
cnn_acc = train_and_evaluate(cnn_model, "CNN", train_loader, test_loader)

print("\nè®­ç»ƒLSTMæ¨¡å‹...")
lstm_model = EmotionLSTM(vocab_size)
lstm_acc = train_and_evaluate(lstm_model, "LSTM", train_loader, test_loader)

# ==================== 7. ç»“æœå¯¹æ¯” ====================
print("\næ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
print(f"CNNæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {cnn_acc:.4f}")
print(f"LSTMæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {lstm_acc:.4f}")

print("\nCNNæµ‹è¯•é›†è¯¦ç»†è¯„ä¼°:")
cnn_model.load_state_dict(torch.load('best_CNN_model.pth'))
full_evaluate(cnn_model, test_loader, label_encoder)

print("\nLSTMæµ‹è¯•é›†è¯¦ç»†è¯„ä¼°:")
lstm_model.load_state_dict(torch.load('best_LSTM_model.pth'))
full_evaluate(lstm_model, test_loader, label_encoder)


# ==================== 8. é¢„æµ‹ç¤ºä¾‹ ====================
def predict_emotion(text, model, vocab, max_len=100):
    """å•æ¡æ–‡æœ¬é¢„æµ‹å‡½æ•°"""
    model.eval()
    text = clean_text(text)
    sequence = [vocab.get(word, vocab["<unk>"]) for word in text.split()]
    sequence = sequence[:max_len] + [0] * (max_len - len(sequence))
    tensor = torch.tensor(sequence).unsqueeze(0)
    with torch.no_grad():
        output = model(tensor.to(next(model.parameters()).device))
    return label_encoder.classes_[torch.argmax(output).item()]


test_samples = [
    "I'm so happy with this product!",
    "This is neither good nor bad",
    "Terrible experience, never buy again"
]

print("\né¢„æµ‹ç¤ºä¾‹:")
for text in test_samples:
    cnn_pred = predict_emotion(text, cnn_model, vocab)
    lstm_pred = predict_emotion(text, lstm_model, vocab)
    print(f"æ–‡æœ¬: {text[:30]}...")
    print(f"  CNNé¢„æµ‹: {cnn_pred:<8} LSTMé¢„æµ‹: {lstm_pred}")